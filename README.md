# Multimodal AI Research Assistant

## ğŸ“Œ Overview

This project implements a **Multimodal AI Research Assistant** capable of understanding and processing multimodal inputsâ€”text, images, and audio. It retrieves relevant information, generates insightful responses, and maintains contextual memory over long interactions.

### âœ¨ Core Features
- **Agentic AI Framework**: Specialized agents for tasks like retrieval, summarization, and generation collaborate for complex goals.
- **LLMs with RAG**: Retrieval-Augmented Generation using GPT-4 or Claude ensures up-to-date, accurate responses.
- **Computer Vision (VLM)**: Vision-Language Models (e.g., BLIP-2) to understand and answer questions about images.
- **Diffusion Models**: Generate images from text prompts using Stable Diffusion.
- **Memory-Augmented Control (MCP)**: Retains memory of previous interactions and user preferences.

---

## ğŸ§  Technologies Used

- **LangChain / LlamaIndex** â€“ Agent management + RAG
- **OpenAI GPT-4 / Claude** â€“ Large Language Models
- **BLIP-2** â€“ Vision-language model for image understanding
- **Stable Diffusion** â€“ Text-to-image generation
- **Python, PyTorch, Transformers**
- **Docker** â€“ For scalable deployment
- **NVIDIA Jetson Orin** â€“ Edge deployment

---

## ğŸ“ Project Structure

multimodal_ai_assistant/
â”‚
â”œâ”€â”€ agents/                    # Agent modules (future use)
â”œâ”€â”€ core/                      # Core logic and memory
â”‚   â”œâ”€â”€ controller.py          # Central controller for managing agents
â”‚   â”œâ”€â”€ memory.py              # Memory management for past interactions
â”œâ”€â”€ models/                    # Model wrappers for LLM, Vision, Diffusion
â”‚   â”œâ”€â”€ llm_rag.py             # LLM with Retrieval-Augmented Generation
â”‚   â”œâ”€â”€ vlm_blip.py            # Vision-Language Model integration (BLIP)
â”‚   â”œâ”€â”€ diffusion_gen.py       # Diffusion model for image generation
â”œâ”€â”€ data/                      # For any external datasets or files
â”œâ”€â”€ main.py                    # Entry point for the assistant
â”œâ”€â”€ requirements.txt           # List of required Python libraries
â”œâ”€â”€ Dockerfile                 # Optional Docker container setup
â””â”€â”€ .env                       # Store your API key (OpenAI)


## Install Dependencies

pip install -r requirements.txt


## ğŸ”‘ Configuration

OPENAI_API_KEY=your-openai-key-here


##  Running the Project . . . 

In the PyCharm terminal or command line, activate your virtual environment and run:
python main.py

You will see:
[Mode: text / image / generate / exit]:


## ğŸ“„ License
This project is licensed under the MIT License.


